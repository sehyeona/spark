# Chapter04 구조적 API 개요
- - -
> 다양한 데이터를 쉽게 다룰 수 있는 RDD와는 다른 *구조적*인 API   
> 데이터의 흐름을 정리하는 기본 추상화 개념
1. Dataset
2. DataFrame
3. SQL 테이블과 뷰
- - - 
### 4.1 DataFrame과 Dataset
> DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션
* 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야한다.
* 컬렉션의 모든 로우는 같은 데이터 타입의 정보를 가지고 있어야한다. 
* DataFrame과 Dataset은 지연 연산의 *실행계획* 이다.(어떤 데이터에 어떤 연산)
* 불변성을 가진다.
#### 스키마
> 스키마는 DataFrame의 컬럼명과 데이터 타입을 정의한다. 
* 데이터 소스에서 얻거나, 직접 정의가능
* 여러 데이터 타입으로 구성(컬럼마다 다르니까), 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법 필요
#### 카탈리스트
> 스파크는 계획수립/ 처리에 사용하는 자체 데이터 타입을 가지고 있는 *카탈리스트 엔진* 을 제공한다.
* 스파크는 자체적인 데이터 타입을 가지고 있기때문에 사실상 프로그래밍 언어이다. 
* 제공하는 다른 언어에 대한 매핑 테이블을 가지고 있다. 
```
df = spark.range(100).toDF('number')
df.select(df['number'] + 50)
```
파이썬으로 작성된 표현식 => 탈리스트 엔진 => 스파크 데이터 타입으로 변형
- - -
#### DataFrmae VS Dataset
![dfds](https://user-images.githubusercontent.com/60355414/84340839-9178f680-abdc-11ea-91f3-acda2af2f65e.PNG)
* Dataset
1. JVM 기반, 파이썬과 R에서는 사용불가(파이썬의 데이터 타입이 불명확하기때문)
2. 모든 데이터 타입이 사용하는 언어(java, scala)로 명확하게 정의되게 때문에, 컴파일 타임에 에러 확인가능
* DataFrame
1. Row 타입으로 구성된 Dataset   
Dataset은 row로 구성된것이 아닌, 명확하게 정의된 데이터 타입의 컬렉션이었는데, 이것을 로우 단위로 모아서 만든것이 DataFrame
2. Row 타입은 스파크연산에 최적화된 인메모리 포맷의 구조 
3. 인메모리 : 텅스텐엔진을 사용 모든 객체를 2진수로 변환후 메모리에서 직접 참조(훨씬 적은 메모리, 빠른 성능)
- - -
#### 컬럼
1. 단순데이터 타입
2. 복합데이터 타입
3. null
#### 로우 
1. 로우는 데이터의 레코드
2. SLQ, RDD, 데이터 소스에서 직접 얻거나 생성가능
```
spark.range(2).collect() 
```
Row 객체로 이루어진 배열을 반환한다.
- - -
### 4.2 스파크의 데이터 타입
```
from pyspark.sql.types import 
```
* 파이썬 데이터 타입 매핑
![dt1](https://user-images.githubusercontent.com/60355414/84341578-6a232900-abde-11ea-8c06-817c02f87efc.PNG)
![dt2](https://user-images.githubusercontent.com/60355414/84341579-6b545600-abde-11ea-8230-8ed9e428798f.PNG)
http://bit.ly/2EdflXW
- - - 
### 4.3 구조적 API의 실행과정 
1. DataFrame/ SQL/ Dataset을 이용한 코드작성
2. 정상적인 코드라면 *논리적 실행계획* 생성
3. 논리적 실행계획을 *물리적 실행계획* 으로 변경 및 *추가 최적화*
4. 클러스터에서 물리적 실행계획 즉 *RDD처리* 실행
![plab](https://user-images.githubusercontent.com/60355414/84341799-fcc3c800-abde-11ea-84b0-e46661392ea1.PNG)
#### 논리적 실행계획
1. 검증전 논리적 실행계획
* 코드를 넘겨받아 추상적인 트랜스포메이션만 진행(표현식을 최적화된 버전으로 바꿈)
* 익스큐터나 드라이버등 물리적인 자원에 대해서는 고려하지 않음
* 코드위 유효성, 컬럼/테이블의 존재여부만 확인
2. 검증된 논리적 실행계획
* 스파크 분석기 이용 카탈로그와 비교하며 컬럼과 테이블의 검증
3. 최적화된 논리적 실행계획
* 카탈리스트를 이용하여 실행계획 최적화
#### 물리적 실행계획
1. '최적화된 논리적 실행계획 + 클러스터환경 확인' 을 통해 다양한 후보의 물리적 실행계획 생성
2. 다양한 물리적 실행계획에서 비용 계산후 최적의 물리적 실행계획 선정 
#### RDD 처리
1. DataFrame등을 RDD로 컴파일(이 과정때문에 스파크를 컴파일러라고도 한다)
2. 전체 task나 stage를 제거할 수 있는 추가적이 최적화
3. 클러스터에서 RDD 계산
